name: "NextformerDiarizer"
num_workers: 18
batch_size: 4

model: 
  sample_rate: 16000
  oracle_mode: False
  pil_weight: 0.5 # Weight for Permutation Invariant Loss (PIL) used in training the Nextformer diarizer model
  ats_weight: 0.5 # Weight for Arrival Time Sort (ATS) loss in training the Nextformer diarizer model
  q_sim_weight: 0.0 # Weight for Query Similarity loss
  emb_sim_weight: 0.0 # Weight for Embedding Similarity loss
  q_contrastive_weight: 0.0 # Weight for Contrastive Query Similarity loss
  q_contrastive_temperature: 0.1 # Temperature for Contrastive Query Similarity loss
  q_contrastive_extra_positive: false # Use an explicit 0.5 margin in contrastive loss
  q_contrastive_extra_positive_rate: 0.1 # Rate for extra positive class in contrastive loss
  q_contrastive_min_frames_anchor: 5 # Minimum number of active frames per query for contrastive loss to be a valid anchor
  q_contrastive_min_frames_positive: 10 # Minimum number of active frames per query for contrastive loss to be a candidate for a positive sample
  q_contrastive_duration_averaged: true # Use duration-averaging for contrastive loss
  q_contrastive_max_frames: 32 # Maximum number of active frames per query for contrastive loss to be used as a weight for duration-averaged loss

  local_num_spks: 4 # Number of speakers for local queries
  max_num_spks: 8 # Maximum number of speakers per model; currently set to 8
  initialize_queries: true # If True, use mean of encoder embeddings over local predictions as initial queries, otherwise use zeros.
  initialize_mask: true # If True, use local predictions as initial encoder mask, otherwise use. (not recommended setting to False!)
  dual_decoder_mode: "concat" # "concat" or "init" or "init_concat"
  streaming_mode: False
  pil_metric: "dot_product"

  model_defaults:
    fc_d_model: 512 # Hidden dimension size of the Fast-conformer Encoder
    tf_d_model: 192 # Hidden dimension size of the Transformer Encoder
    sq_d_model: 192 # Hidden dimension size of the Spk Query Decoder

  train_ds:
    manifest_filepath: ???
    sample_rate: ${model.sample_rate}
    num_spks: 8
    session_len_sec: 90 # Maximum session length in seconds
    soft_label_thres: 0.5 # Threshold for binarizing target values; higher values make the model more conservative in predicting speaker activity.
    soft_targets: False # If True, use continuous values as target values when calculating cross-entropy loss
    labels: null
    batch_size: ${batch_size}
    shuffle: True
    num_workers: ${num_workers}
    validation_mode: False
    # lhotse config
    num_speakers: 8
    use_lhotse: False
    use_bucketing: True
    num_buckets: 10
    bucket_duration_bins: [10, 20, 30, 40, 50, 60, 70, 80, 90]
    pin_memory: True
    min_duration: 10
    max_duration: 90
    batch_duration: 400
    quadratic_duration: 1200
    bucket_buffer_size: 20000
    shuffle_buffer_size: 10000
    window_stride: ${model.preprocessor.window_stride}
    subsampling_factor: ${model.encoder.subsampling_factor}

  validation_ds:
    manifest_filepath: ???
    is_tarred: False
    tarred_audio_filepaths: null
    sample_rate: ${model.sample_rate}
    num_spks: 8
    num_speakers: 8
    session_len_sec: 90 # Maximum session length in seconds
    soft_label_thres: 0.5 # A threshold value for setting up the binarized labels. The higher the more conservative the model becomes. 
    soft_targets: False
    labels: null
    batch_size: ${batch_size}
    shuffle: False
    num_workers: ${num_workers}
    validation_mode: True
    # lhotse config
    use_lhotse: False
    use_bucketing: False
    drop_last: False
    pin_memory: True
    window_stride: ${model.preprocessor.window_stride}
    subsampling_factor: ${model.encoder.subsampling_factor}
  
  test_ds:
    manifest_filepath: null
    is_tarred: False
    tarred_audio_filepaths: null
    sample_rate: 16000
    num_spks: 8
    num_speakers: 8
    session_len_sec: 90 # Maximum session length in seconds
    soft_label_thres: 0.5
    soft_targets: False
    labels: null
    batch_size: ${batch_size}
    shuffle: False
    seq_eval_mode: True
    num_workers: ${num_workers}
    validation_mode: True
    # lhotse config
    use_lhotse: False
    use_bucketing: False
    drop_last: False
    pin_memory: True
    window_stride: ${model.preprocessor.window_stride}
    subsampling_factor: ${model.encoder.subsampling_factor}

  preprocessor:
    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor
    normalize: "NA"
    window_size: 0.025
    sample_rate: ${model.sample_rate}
    window_stride: 0.01
    window: "hann"
    features: 128
    n_fft: 512
    frame_splicing: 1
    dither: 0.00001

  nextformer_modules:
    _target_: nemo.collections.asr.modules.nextformer_modules.NextformerModules
    ff_dropout_rate: 0.5 # Dropout rate
    fc_d_model: ${model.model_defaults.fc_d_model} # Hidden dimension size for Fast Conformer encoder
    tf_d_model: ${model.model_defaults.tf_d_model} # Hidden dimension size for Transformer encoder
    sq_d_model: ${model.model_defaults.sq_d_model} # Hidden dimension size for Spk Query decoder
    local_num_spks: ${model.local_num_spks} # Number of speakers for local queries
    max_num_spks: ${model.max_num_spks} # Maximum number of speakers per model
    pred_score_threshold: 0.25 # Threshold for speaker predictions to be used in scoring
    chunk_len: 125 # Length of each chunk
    chunk_left_context: 62 # Left context for each chunk
    chunk_right_context: 1 # Right context for each chunk
    causal_attn_rate: 0 # Causal attention rate
    causal_attn_rc: 7 # Causal attention radius

  encoder:
    _target_: nemo.collections.asr.modules.ConformerEncoder
    feat_in: ${model.preprocessor.features}
    feat_out: -1
    n_layers: 17
    d_model: ${model.model_defaults.fc_d_model}

    # Sub-sampling parameters
    subsampling: dw_striding # vggnet, striding, stacking or stacking_norm, dw_striding
    subsampling_factor: 8 # must be power of 2 for striding and vggnet
    subsampling_conv_channels: 256 # set to -1 to make it equal to the d_model
    causal_downsampling: false
    # Feed forward module's params
    ff_expansion_factor: 4
    # Multi-headed Attention Module's params
    self_attention_model: rel_pos # rel_pos or abs_pos
    n_heads: 8 # may need to be lower for smaller d_models
    # [left, right] specifies the number of steps to be seen from left and right of each step in self-attention
    att_context_size: [-1, -1] # -1 means unlimited context
    att_context_style: regular # regular or chunked_limited
    xscaling: true # scales up the input embeddings by sqrt(d_model)
    untie_biases: true # unties the biases of the TransformerXL layers
    pos_emb_max_len: 5000
    # Convolution module's params
    conv_kernel_size: 9
    conv_norm_type: 'batch_norm' # batch_norm or layer_norm or groupnormN (N specifies the number of groups)
    conv_context_size: null
    # Regularization
    dropout: 0.1 # The dropout used in most of the Conformer Modules
    dropout_pre_encoder: 0.1 # The dropout used before the encoder
    dropout_emb: 0.0 # The dropout used for embeddings
    dropout_att: 0.1 # The dropout for multi-headed attention modules
    # Set to non-zero to enable stochastic depth
    stochastic_depth_drop_prob: 0.0
    stochastic_depth_mode: linear  # linear or uniform
    stochastic_depth_start_layer: 1
  
  transformer_encoder:
    _target_: nemo.collections.asr.modules.transformer.transformer_encoders.TransformerEncoder
    num_layers: 18
    hidden_size: ${model.model_defaults.tf_d_model} # Needs to be multiple of num_attention_heads
    inner_size: 768
    num_attention_heads: 8
    attn_score_dropout: 0.5
    attn_layer_dropout: 0.5
    ffn_dropout: 0.5
    hidden_act: relu
    pre_ln: False
    pre_ln_final_layer_norm: True

  query_decoder:
    _target_: nemo.collections.asr.modules.nextformer_modules.MaskedQueryDecoder
    num_layers: 4
    hidden_size: ${model.model_defaults.sq_d_model} # Needs to be multiple of num_attention_heads
    inner_size: 768
    num_queries: ${model.local_num_spks}
    extra_cross_attention: False
    num_cross_attention_heads: 4
    num_self_attention_heads: 4
    cross_attn_dropout: 0.0
    self_attn_dropout: 0.0
    ffn_dropout: 0.0
    hidden_act: relu
    pre_ln: False
    pre_ln_final_layer_norm: True
    use_query_pos_emb: False
    use_encoder_pos_emb: False
    encoder_pos_emb_max_len: 5000

  query_decoder_raw:
    _target_: nemo.collections.asr.modules.nextformer_modules.MaskedQueryDecoder
    num_layers: 4
    hidden_size: ${model.model_defaults.sq_d_model} # Needs to be multiple of num_attention_heads
    inner_size: 768
    num_queries: ${model.local_num_spks}
    extra_cross_attention: False
    num_cross_attention_heads: 4
    num_self_attention_heads: 4
    cross_attn_dropout: 0.0
    self_attn_dropout: 0.0
    ffn_dropout: 0.0
    hidden_act: relu
    pre_ln: False
    pre_ln_final_layer_norm: True
    use_query_pos_emb: False
    use_encoder_pos_emb: False
    encoder_pos_emb_max_len: 5000

  loss: 
    _target_: nemo.collections.asr.losses.bce_loss.BCEWithLogitsLoss
    reduction: mean

  emb_sim_loss:
    _target_: torch.nn.SmoothL1Loss
    reduction: mean
    beta: 0.5

  q_sim_loss:
    _target_: torch.nn.CosineEmbeddingLoss
    margin: 0.0
    reduction: 'mean'

  lr: 0.0001
  optim:
    name: adamw
    lr: ${model.lr}
    # optimizer arguments
    betas: [0.9, 0.98]
    weight_decay: 1e-3

    sched:
      name: InverseSquareRootAnnealing
      warmup_steps: 500
      warmup_ratio: null
      min_lr: 1e-06

trainer:
  devices: 1 # number of gpus (devices)
  accelerator: gpu
  precision: 32 # 32, bf16, bf16-mixed
  max_epochs: 800
  max_steps: -1 # computed at runtime if not set
  num_nodes: 1
  strategy: ddp_find_unused_parameters_true # Could be "ddp"
  accumulate_grad_batches: 1
  deterministic: True
  enable_checkpointing: False
  logger: False
  log_every_n_steps: 1  # Interval of logging.
  val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations

exp_manager:
  use_datetime_version: False
  exp_dir: null
  name: ${name}
  resume_if_exists: True
  resume_from_checkpoint: null # The path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.
  resume_ignore_no_checkpoint: True
  create_tensorboard_logger: True
  create_checkpoint_callback: True
  create_wandb_logger: False
  checkpoint_callback_params:
    monitor: "val_f1_acc_global"
    mode: "max"
    save_top_k: 9
    every_n_epochs: 1
  wandb_logger_kwargs:
    resume: True
    name: null
    project: null